---
title: "Final Project- STAT 425. COVID-19 Healthy Diet and Recovery Rate"
output: html_document
author: "Sandip Sonawane (sandip2@illinois.edu), Nursultan Bailessov (nb17@illinois.edu)"
date: "May 30, 2021"
output:
  html_document: 
    theme: default
    toc: yes
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Praparing training and testing data for Linear Model
```{r}
#Preparing the training and testing data
df <- read.csv("Food_Supply_kcal_Data.csv", header = TRUE)
df = data.frame(df)
rownames(df) <- df$Country
df$Country = NULL
df$Population = NULL
df$Deaths = NULL
df$Confirmed = NULL
df$Active = NULL
df$Undernourished = NULL
df$Unit..all.except.Population. = NULL

df = df[-c(53,81,82,106,110,156),]
set.seed(100)
dt = sort(sample(nrow(df), nrow(df)*.8))
traindata <- df[dt,]
testdata <- df[-dt,]
```

## Performing linear regression
```{r}
library(car)
library(MASS)
library(lmtest)
library(sandwich)

traindataLM = traindata
traindataLM$Animal.Products = NULL
traindataLM$Vegetal.Products = NULL

traindataLM$Aquatic.Products..Other = NULL
traindataLM$Fish..Seafood = NULL
traindataLM$Miscellaneous = NULL
traindataLM$Fruits...Excluding.Wine = NULL
traindataLM$Spices = NULL
traindataLM$Sugar.Crops = NULL

lin_reg1 = lm(Recovered ~ ., data = traindataLM)
vif(lin_reg1)
traindataLM$Cereals...Excluding.Beer = NULL

lin_reg2 = lm(Recovered ~ ., data = traindataLM)
vif(lin_reg2)
summary(lin_reg2)

#Linear regression diagnostics
par(mfrow=c(2,2))
plot(lin_reg2)
shapiro.test(lin_reg2$res)

lin_reg3 = lm((Recovered + 1) ~ ., data=traindataLM)
boxcox(lin_reg3)

lin_reg4 = lm(1/(Recovered + 1) ~ ., data=traindataLM)
summary(lin_reg4)
par(mfrow=c(2,2))
plot(lin_reg4)
shapiro.test(lin_reg4$res)
bptest(lin_reg4)

#we have heteroskedasticity that means that our ordinary least square estimators are no longer the best and standard errors could be misleading, so we use robust standard errors
#we have sufficiently large sample size, so we expect that the results of OLS are precise enough
coeftest(lin_reg4, vcov = vcovHC(lin_reg4, "HC1"))
#there is no big difference in significance of variables after using robust standard errors, as expected, so we can use usual standard errors

rmse <- function(x,y) sqrt(mean((x-y)^2))
RMSEtrain = sqrt(mean(lin_reg4$res**2))
RMSEtrain
RMSEtest = rmse(1/(testdata$Recovered + 1), predict(lin_reg4, testdata))
RMSEtest
```

## Performing linear regression with variables selected using BIC by using stepwise elimination
```{r}
library(leaps)
n = dim(traindata)[1]
msize = 2:16
BICreg1 = regsubsets(1/(Recovered + 1) ~ ., data =traindataLM, nvmax = 15)
rs = summary(BICreg1)
rs$which
BIC <- n*log(rs$rss/n) + msize*log(n)
plot(msize, BIC, xlab = "No. of Parameters", ylab = "BIC")
which.min(BIC) + 1

BICreg2 = lm(1/(Recovered +  1) ~ Oilcrops + Stimulants + Obesity, data = traindataLM)
summary(BICreg2)
RMSEtrain = sqrt(mean(BICreg2$res**2))
RMSEtrain
RMSEtest = rmse(1/(testdata$Recovered + 1), predict(BICreg2, testdata))
RMSEtest
```
## Performing linear regression with variables selected using AIC by using stepwise elimination
```{r}
g <- lm(1/(Recovered + 1) ~ ., data = traindataLM)
step(g)
AICreg = lm(1/(Recovered + 1) ~ Animal.fats + Eggs + Meat + Oilcrops + Stimulants + Obesity, data = traindataLM)
summary(AICreg)
RMSEtrain = sqrt(mean(AICreg$res**2))
RMSEtrain
RMSEtest = rmse(1/(testdata$Recovered + 1), predict(AICreg, testdata))
RMSEtest
```

## Performing Lasso regression
```{r}
#Performing Lasso regression
library(lars)

trainx <- model.matrix( ~ . - 1, data = traindataLM[, !(names(traindataLM) %in% c("Recovered"))])
traindataLM2 = traindataLM[-115, ]
trainy <- as.vector(1/(traindataLM2$Recovered + 1))

testx = model.matrix( ~ . -1, data = testdata[, !(names(testdata) %in% c("Recovered", "Animal.Products", "Vegetal.Products", "Aquatic.Products..Other", "Fish..Seafood", "Miscellaneous", "Fruits...Excluding.Wine", "Spices", "Sugar.Crops", "Cereals...Excluding.Beer"))])
testy = as.vector(1/(testdata$Recovered + 1))

lassoReg <- lars(trainx, trainy)
set.seed(100)
cvml <- cv.lars(trainx, trainy)
svm <- cvml$index[which.min(cvml$cv)]
svm

TRpredlasso <- predict(lassoReg, trainx, s=svm, mode = "fraction")

predlasso <- predict(lassoReg, testx, s=svm, mode = "fraction")
round(predict(lassoReg, s=svm, type = "coef", mode = "fraction")$coef, 3)
predlasso$fit

RMSEtrain = rmse(trainy, TRpredlasso$fit)
RMSEtrain
RMSEtest = rmse(1/(testdata$Recovered + 1), predlasso$fit)
RMSEtest
```

## Performing Principal Components Regression
```{r}
library(pls)
set.seed(100)
modpcrcv = pcr(1/(Recovered + 1) ~ ., data=traindata, validation="CV", ncomp=22)
pcrCV = RMSEP(modpcrcv, estimate="CV")
plot(pcrCV, col = 'blue')
which.min(pcrCV$val) - 1

RMSEtrain = sqrt(mean(modpcrcv$res**2))
RMSEtrain
RMSEtest = rmse(1/(testdata$Recovered + 1), predict(modpcrcv, testdata, ncomp = 22))
RMSEtest
```



## Using Non-Parametric Regression Models

```{r}
# Import Libraries
library(corrplot)
library(readr)
library(ggplot2)
library(caret)
library(MLmetrics)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
library(dplyr)
```


```{r}
# import data
df <- read.csv(file = 'Food_Supply_kcal_Data.csv', header = TRUE)
df = data.frame(df)
head(df, 3)
```


```{r}
names(df)
```

Since last column is just a % unit, we can drop it.


```{r}
df = select(df, -Unit..all.except.Population.)
```

* There are 6 row items which have value NA for columns confirmed, deaths, recovered, and active, lets remove these rows since data from these countries is absent.


```{r}
df = df[-c(53,81,82, 106, 110, 156),]
```


```{r}
new_df = select(df, Confirmed, Recovered, Active, Deaths)
cor(new_df, use = "na.or.complete")
```

* Confirmed, Deaths, Recovered, and Active are predictions. Deaths will be highly correlated with Confirmed, Recovered, and Active, so we will not consider them as predictors. Instead, along with Deaths, a new variable recovery rate can be a good prediction.

$ Recovery Rate = \frac{Recoverd}{Confirmed-Active}$


```{r}
# df$RecoverRate = df$Recovered/(df$Confirmed)
# df = df[!is.na(df$RecoverRate),]
df = df[!is.na(df$Obesity),]
```


```{r}
data = round(cor(select(df, c(-Country, -Undernourished, -Population, -Active,
                              -Confirmed, -Deaths)),
                 use = "na.or.complete"), 2)
```


```{r}
library(reshape2)
melted_df <- melt(data)
head(melted_df)
```



```{r}
library(corrplot)
residuals <- cor.mtest(data, conf.level = .95)

options(repr.plot.width=11, repr.plot.height=10)
corrplot(data, order = "hclust", type = 'upper',
         tl.col = "slategray", tl.srt = 45, tl.cex = 0.75, p.mat = residuals$p, sig.level = .01)
```

    


* We can see some patterns coming out of the correlation matrix
* Vegetal Products and Animal Products (Milk, Meat, Eggs, Animal Fats) have strong negative correlation

### Drop variables that do not have direct relationship with recovery rate.


```{r}
data_new = round(cor(select(df, c(-Sugar.Crops, -Fruits...Excluding.Wine, -Vegetables, - Aquatic.Products..Other,
                              - Fish..Seafood, -Offals, -Country, -Undernourished, -Population, - Active,-Confirmed, -Deaths, -Spices,
                                 -Sugar.Crops, -Fruits...Excluding.Wine)),
                 use = "na.or.complete"), 2)
```


```{r}
options(repr.plot.width=11, repr.plot.height=5)
corrplot(data_new, order = "hclust", type = 'upper',
         tl.col = "slategray", tl.srt = 45, tl.cex = 0.75, sig.level = .05)
```


    

* From above plot, we can remove those variables that have high correlation with each other. This will ensure inverse of our design matrix exists.
* We can observe below:
1. Animal.Products (has high negative correlation with Vegetal.Products)
2. Meat (has a high negative correlation with Vegetal.Products)
3. Milk...Excluding.Butter (has high correlation with Vegetal.Products)
4. Animal.fats (has high correlation with Vegetal.Products)

Since Animal products and vegetal products are a combination of many variables, we will drop these two.


```{r}
data_new = round(cor(select(df, c(- Aquatic.Products..Other,
                              - Fish..Seafood, -Miscellaneous, -Country, -Undernourished, -Population, - Active,-Vegetal.Products,
                              -Confirmed, -Deaths, -Animal.Products, -Spices, -Sugar.Crops, -Fruits...Excluding.Wine)),
                 use = "na.or.complete"), 2)
```


```{r}
residuals <- cor.mtest(data_new, conf.level = .95)

options(repr.plot.width=11, repr.plot.height=10)
corrplot(data_new, order = "hclust", type = 'upper',
         tl.col = "slategray", tl.srt = 45, tl.cex = 0.75)
```



```{r}
library(ggplot2)
options(repr.plot.width=11, repr.plot.height=3)

pltColors = c('Alcoholic.Beverages' = 'red', 'Animal.Products' = 'blue', 'Milk...Excluding.Butter' = 'green',
              'Fruits...Excluding.Wine' = 'orange', 'Sugar...Sweeteners' = '#fcba03', 'Cereals...Excluding.Beer' = 'grey',
              'Meat'='red', 'Vegetal.Products'='red', 'Pulses'='red', 'Starchy.Roots'='red'
             )

ggplot(df, aes(x=1, y=Alcoholic.Beverages, fill = 'Alcoholic.Beverages')) +
  theme_update(plot.title = element_text(hjust = 0.5))+
  theme_set(theme_bw())+
  theme(text = element_text(size=14))+
  scale_x_continuous(breaks = seq(1, 10, by = 1))+
#   coord_cartesian(ylim = c(-20, 15))+
  labs(title='Boxplots of variables', x='Variable Number', y = '% inclusion in diet')+
  scale_colour_manual(values=pltColors)+
  geom_boxplot(outlier.shape = NA)+

  geom_boxplot(aes(x=2, y=Animal.Products, fill='Animal.Products'), outlier.shape = NA)+
  geom_boxplot(aes(x=3, y=Milk...Excluding.Butter, fill='Milk...Excluding.Butter'), outlier.shape = NA)+
  geom_boxplot(aes(x=4, y=Fruits...Excluding.Wine, fill='Fruits...Excluding.Wine'), outlier.shape = NA)+
  geom_boxplot(aes(x=5, y=Sugar...Sweeteners, fill='Sugar...Sweeteners'), outlier.shape = NA)+
  geom_boxplot(aes(x=6, y=Cereals...Excluding.Beer, fill='Cereals...Excluding.Beer'), outlier.shape = NA)+
  geom_boxplot(aes(x=7, y=Meat, fill='Meat'), outlier.shape = NA)+
  geom_boxplot(aes(x=8, y=Vegetal.Products, fill='Vegetal.Products'), outlier.shape = NA)+
  geom_boxplot(aes(x=9, y=Pulses, fill='Pulses'), outlier.shape = NA)+
  geom_boxplot(aes(x=10, y=Starchy.Roots, fill='Starchy.Roots'), outlier.shape = NA)
```


```{r}
sort(names(df))
```



```{r}
options(repr.plot.width=10, repr.plot.height=3)

ggplot(df, aes(x = Recovered))+
    theme_update(plot.title = element_text(hjust = 0.5))+
    theme_set(theme_bw())+
    scale_x_continuous(breaks = seq(-0, 10, by = 0.5))+
    theme(text = element_text(size=12))+
    geom_density(fill = 'darkorange')+
    labs(title='Distribution Recovery Rate', x='Recovery Rate', y = 'density')
```


    


## Linear Regression Model.


```{r}
data_raw = select(df, c(-Country, -Undernourished, -Population, -Active,
                              -Confirmed, -Deaths, -Animal.Products, -Vegetal.Products, -Fish..Seafood, -Miscellaneous,
                       -Aquatic.Products..Other, -Fruits...Excluding.Wine, -Animal.Products, -Milk...Excluding.Butter))
```


```{r}
trn_idx = sample(nrow(data_raw), size = 0.8 * nrow(data_raw))
trn_data = data_raw[trn_idx, ]
tst_data = data_raw[-trn_idx, ]
```


```{r}
column_NAs = function(x){
  sum(is.na(trn_data[,x]))
}
NAs = sapply(1:ncol(trn_data), column_NAs)
NAs
```


```{r}
lin_mod_a = lm(1/ (Recovered+1) ~ .,
               data = trn_data)
summary(lin_mod_a)
train_RMSE = sqrt(mean(lin_mod_a$residuals**2))
train_RMSE
test_RMSE = calc_RMSE(predict(lin_mod_a, tst_data), 1/(tst_data$Recovered+1))
test_RMSE
```


    
    Call:
    lm(formula = 1/(Recovered + 1) ~ ., data = trn_data)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -0.57831 -0.13821  0.01277  0.12058  0.57939 
    
    Coefficients:
                              Estimate Std. Error t value Pr(>|t|)   
    (Intercept)               0.662363   0.521350   1.270  0.20667   
    Alcoholic.Beverages      -0.034068   0.030424  -1.120  0.26532   
    Animal.fats              -0.047523   0.023253  -2.044  0.04344 * 
    Cereals...Excluding.Beer  0.003284   0.010425   0.315  0.75335   
    Eggs                     -0.198189   0.093972  -2.109  0.03728 * 
    Meat                      0.045674   0.018750   2.436  0.01650 * 
    Offals                    0.067261   0.215640   0.312  0.75571   
    Oilcrops                  0.039185   0.021291   1.840  0.06847 . 
    Pulses                    0.014965   0.024131   0.620  0.53647   
    Spices                    0.032139   0.092752   0.347  0.72965   
    Starchy.Roots             0.009585   0.012303   0.779  0.43768   
    Stimulants               -0.028276   0.090238  -0.313  0.75463   
    Sugar.Crops               0.374974   0.351221   1.068  0.28809   
    Sugar...Sweeteners       -0.008180   0.018823  -0.435  0.66475   
    Treenuts                 -0.098389   0.080064  -1.229  0.22181   
    Vegetable.Oils            0.014783   0.014492   1.020  0.31000   
    Vegetables               -0.006508   0.042120  -0.155  0.87750   
    Obesity                  -0.010802   0.003260  -3.314  0.00126 **
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    Residual standard error: 0.2242 on 107 degrees of freedom
    Multiple R-squared:  0.5558,	Adjusted R-squared:  0.4852 
    F-statistic: 7.875 on 17 and 107 DF,  p-value: 2.213e-12
    




```{r}
lin_mod_b = lm(1/ (Recovered+1) ~ Meat + Oilcrops + Stimulants + Obesity + Eggs,
               data = trn_data)
summary(lin_mod_b)

train_RMSE = sqrt(mean(lin_mod_b$residuals**2))
train_RMSE
test_RMSE = calc_RMSE(predict(lin_mod_b, tst_data), 1/(tst_data$Recovered+1))
test_RMSE
sqrt(mean(lin_mod_b$residuals**2))
```


    
    Call:
    lm(formula = 1/(Recovered + 1) ~ Meat + Oilcrops + Obesity + 
        Eggs, data = trn_data)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -0.42009 -0.17548  0.00408  0.14644  0.56907 
    
    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  0.907099   0.056017  16.193  < 2e-16 ***
    Meat         0.027308   0.011609   2.352  0.02029 *  
    Oilcrops     0.047558   0.016201   2.936  0.00399 ** 
    Obesity     -0.015712   0.002652  -5.923 3.08e-08 ***
    Eggs        -0.371817   0.081386  -4.569 1.20e-05 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    Residual standard error: 0.2315 on 120 degrees of freedom
    Multiple R-squared:  0.4689,	Adjusted R-squared:  0.4512 
    F-statistic: 26.49 on 4 and 120 DF,  p-value: 9.414e-16
    



```{r}
anova(lin_mod_b, lin_mod_a)
```

## KNN Regression Model



```{r}
set.seed(42)
model_Control = trainControl(method = "cv", number = 5)

knn_model_full = train(
  form = 1/ (Recovered+1) ~.,
  data = trn_data,
  trControl = model_Control,
  method = "knn",
  tuneGrid = expand.grid(k = seq(1, 30, by = 1))
)

knn_model_full
```


    k-Nearest Neighbors 
    
    125 samples
     17 predictor
    
    No pre-processing
    Resampling: Cross-Validated (5 fold) 
    Summary of sample sizes: 100, 100, 101, 100, 99 
    Resampling results across tuning parameters:
    
      k   RMSE       Rsquared   MAE      
       1  0.3086230  0.2477358  0.2308274
       2  0.2879346  0.2649755  0.2262520
       3  0.2675536  0.3256232  0.2079687
       4  0.2641689  0.3125542  0.2081874
       5  0.2601669  0.3345824  0.2072483
       6  0.2609612  0.3286910  0.2101020
       7  0.2570235  0.3417433  0.2082592
       8  0.2524604  0.3553865  0.2039431
       9  0.2515286  0.3630446  0.2040741
      10  0.2494470  0.3737077  0.2040547
      11  0.2494592  0.3773883  0.2032202
      12  0.2504384  0.3754209  0.2041506
      13  0.2506940  0.3686961  0.2040669
      14  0.2473458  0.3847179  0.2012165
      15  0.2470133  0.3859655  0.2006184
      16  0.2456363  0.3941795  0.2006268
      17  0.2455056  0.3970150  0.2013624
      18  0.2430002  0.4112686  0.1996432
      19  0.2438558  0.4071431  0.2000766
      20  0.2434126  0.4075207  0.1988554
      21  0.2453543  0.4025499  0.2021054
      22  0.2470253  0.3953807  0.2036924
      23  0.2475662  0.3934078  0.2043818
      24  0.2482036  0.3897966  0.2057288
      25  0.2476881  0.3938470  0.2047464
      26  0.2501903  0.3803010  0.2078737
      27  0.2507582  0.3767070  0.2084322
      28  0.2505742  0.3798246  0.2077167
      29  0.2505951  0.3790820  0.2075027
      30  0.2501369  0.3818961  0.2084387
    
    RMSE was used to select the optimal model using the smallest value.
    The final value used for the model was k = 18.



```{r}
oneSE(knn_model_full$results, "RMSE", maximize = F, num = 30)
```



```{r}
set.seed(42)
knn_model_reduced = train(
  form = 1/ (Recovered+1) ~ Treenuts+ Alcoholic.Beverages + Oilcrops + Stimulants + Obesity + Eggs,
  data = trn_data,
  trControl = model_Control,
  method = "knn",
  tuneGrid = expand.grid(k = seq(1, 30, by = 1))
)

knn_model_reduced
```


    k-Nearest Neighbors 
    
    125 samples
      6 predictor
    
    No pre-processing
    Resampling: Cross-Validated (5 fold) 
    Summary of sample sizes: 100, 100, 101, 100, 99 
    Resampling results across tuning parameters:
    
      k   RMSE       Rsquared   MAE      
       1  0.3195392  0.2212923  0.2212317
       2  0.2782241  0.3271537  0.2152032
       3  0.2682520  0.3307200  0.2073083
       4  0.2623724  0.3397060  0.2012443
       5  0.2659895  0.3275306  0.2079542
       6  0.2624285  0.3438730  0.2062330
       7  0.2523637  0.3822842  0.2023658
       8  0.2552468  0.3651409  0.2063210
       9  0.2487573  0.3876332  0.2022627
      10  0.2445888  0.4021624  0.2000082
      11  0.2452377  0.4020039  0.2001598
      12  0.2445067  0.4079837  0.1983940
      13  0.2481444  0.3878049  0.2024846
      14  0.2486045  0.3870059  0.2029650
      15  0.2495301  0.3800953  0.2037663
      16  0.2516702  0.3716461  0.2060076
      17  0.2512151  0.3744111  0.2055430
      18  0.2535295  0.3618396  0.2068393
      19  0.2529939  0.3633395  0.2067349
      20  0.2549596  0.3533919  0.2081202
      21  0.2554164  0.3524949  0.2085796
      22  0.2559517  0.3527068  0.2103921
      23  0.2561919  0.3508190  0.2096798
      24  0.2563312  0.3504448  0.2088550
      25  0.2560844  0.3514421  0.2086865
      26  0.2545030  0.3606815  0.2078659
      27  0.2545211  0.3618597  0.2081785
      28  0.2543235  0.3630960  0.2076093
      29  0.2530016  0.3684946  0.2066882
      30  0.2533339  0.3675101  0.2075281
    
    RMSE was used to select the optimal model using the smallest value.
    The final value used for the model was k = 12.



```{r}
calc_RMSE = function(predicted, actual){
    sqrt(mean((predicted - actual)**2))
}
```


```{r}
pred = predict(knn_model_full, tst_data)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```



```{r}
pred = predict(knn_model_reduced, tst_data)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```



```{r}
knn_full_model_rmse = data.frame(knn_model_full$results)
knn_reduce_model_rmse = data.frame(knn_model_reduced$results)

head(knn_full_model_rmse,3)
```




```{r}
options(repr.plot.width=10, repr.plot.height=5)

ggplot(data = knn_full_model_rmse, aes(x = k, y = RMSE, color = 'Full Model')) +
  theme_update(plot.title = element_text(hjust = 0.5))+
  theme_set(theme_bw())+
  theme(text = element_text(size=14))+
  labs(title='KNN model Tuning', x='k', y = 'Cross Validated RMSE')+
  scale_x_continuous(breaks = seq(1, 30, by = 4))+

  geom_line()+
  geom_point()+
    
  geom_line(aes(x = knn_reduce_model_rmse$k, y = knn_reduce_model_rmse$RMSE, color = "Reduced Model"))+
  geom_point(aes(x = knn_reduce_model_rmse$k, y = knn_reduce_model_rmse$RMSE, color = "Reduced Model"))
```



```{r}
# knn_mod_a = knnreg(Recovered ~., data = trn_data)
# calc_RMSE(predict(knn_mod_a, trn_data), trn_data$Recovered)
```


```{r}
# knn_mod_b = knnreg(Recovered ~ Animal.fats+Stimulants+Obesity, data = trn_data)
# calc_RMSE(predict(knn_mod_b, trn_data), trn_data$Recovered)
```


```{r}
# Ks = seq(2, 20, 1)
# create_models = function(kNumber){
#     knnreg(Recovered ~., data = trn_data, k = kNumber)
# }
# create_models_1 = function(kNumber){
#     knnreg(Recovered ~ Animal.fats+Stimulants+Obesity, data = trn_data, k = kNumber)
# }
```


```{r}
# models = lapply(Ks, create_models)
# predictions = lapply(models, predict, tst_data)
# RMSEs = sapply(predictions, calc_RMSE, tst_data$Recovered)
# RMSEs
```


```{r}
# models = lapply(Ks, create_models_1)
# predictions = lapply(models, predict, tst_data)
# RMSEs = sapply(predictions, calc_RMSE, tst_data$Recovered)
# RMSEs
```

* Instead of fitting model with all variables, it is better to fit the model with lesser variables.

## Decision Tree Model


```{r}
set.seed(42)
model_Control = trainControl(method = "cv", number = 5)

decision_tree_full_model = train(
  form = 1/ (Recovered+1) ~ .,
  data = trn_data,
  trControl = model_Control,
  method = "rpart",
  tuneGrid = expand.grid(cp = seq(0, 0.15, 0.005))
)

decision_tree_full_model
```


    CART 
    
    125 samples
     17 predictor
    
    No pre-processing
    Resampling: Cross-Validated (5 fold) 
    Summary of sample sizes: 100, 100, 101, 100, 99 
    Resampling results across tuning parameters:
    
      cp     RMSE       Rsquared   MAE      
      0.000  0.2658556  0.3696466  0.2008034
      0.005  0.2658191  0.3720291  0.2012675
      0.010  0.2665000  0.3713921  0.2016928
      0.015  0.2651504  0.3756580  0.2008188
      0.020  0.2632663  0.3763754  0.2000169
      0.025  0.2626826  0.3781700  0.1992159
      0.030  0.2626826  0.3781700  0.1992159
      0.035  0.2626826  0.3781700  0.1992159
      0.040  0.2652839  0.3704513  0.1999894
      0.045  0.2656474  0.3675637  0.1993722
      0.050  0.2643262  0.3506538  0.2027102
      0.055  0.2723633  0.3089578  0.2134252
      0.060  0.2723633  0.3089578  0.2134252
      0.065  0.2701140  0.3153852  0.2118146
      0.070  0.2701140  0.3153852  0.2118146
      0.075  0.2639001  0.3332551  0.2123586
      0.080  0.2639001  0.3332551  0.2123586
      0.085  0.2639001  0.3332551  0.2123586
      0.090  0.2705996  0.2967000  0.2190173
      0.095  0.2705996  0.2967000  0.2190173
      0.100  0.2705996  0.2967000  0.2190173
      0.105  0.2662403  0.2985941  0.2184950
      0.110  0.2662403  0.2985941  0.2184950
      0.115  0.2662403  0.2985941  0.2184950
      0.120  0.2662403  0.2985941  0.2184950
      0.125  0.2662403  0.2985941  0.2184950
      0.130  0.2662403  0.2985941  0.2184950
      0.135  0.2662403  0.2985941  0.2184950
      0.140  0.2662403  0.2985941  0.2184950
      0.145  0.2662403  0.2985941  0.2184950
      0.150  0.2662403  0.2985941  0.2184950
    
    RMSE was used to select the optimal model using the smallest value.
    The final value used for the model was cp = 0.035.



```{r}
library(vip)
```


```{r}
vip(decision_tree_full_model, num_features = 40, bar = FALSE)
```



```{r}
set.seed(42)
model_Control = trainControl(method = "cv", number = 5)

decision_tree_reduced_model = train(
  form = 1 / (Recovered+1) ~ Animal.fats + Sugar...Sweeteners + Oilcrops + Stimulants + Obesity + Eggs + Treenuts,
  data = trn_data,
  trControl = model_Control,
  method = "rpart",
  tuneGrid = expand.grid(cp = seq(0, 0.15, 0.005))
)

decision_tree_reduced_model
```


    CART 
    
    125 samples
      7 predictor
    
    No pre-processing
    Resampling: Cross-Validated (5 fold) 
    Summary of sample sizes: 100, 100, 101, 100, 99 
    Resampling results across tuning parameters:
    
      cp     RMSE       Rsquared   MAE      
      0.000  0.2665762  0.3472093  0.1996699
      0.005  0.2664687  0.3508183  0.1995066
      0.010  0.2671534  0.3505947  0.1999319
      0.015  0.2658724  0.3546147  0.1990580
      0.020  0.2636568  0.3588233  0.1982560
      0.025  0.2615008  0.3570890  0.1977699
      0.030  0.2615008  0.3570890  0.1977699
      0.035  0.2615008  0.3570890  0.1977699
      0.040  0.2605518  0.3622842  0.1965231
      0.045  0.2609370  0.3584454  0.1986352
      0.050  0.2600556  0.3545144  0.2008717
      0.055  0.2640319  0.3368808  0.2106728
      0.060  0.2640319  0.3368808  0.2106728
      0.065  0.2617826  0.3433082  0.2090622
      0.070  0.2617826  0.3433082  0.2090622
      0.075  0.2639001  0.3332551  0.2123586
      0.080  0.2639001  0.3332551  0.2123586
      0.085  0.2639001  0.3332551  0.2123586
      0.090  0.2705996  0.2967000  0.2190173
      0.095  0.2705996  0.2967000  0.2190173
      0.100  0.2705996  0.2967000  0.2190173
      0.105  0.2662403  0.2985941  0.2184950
      0.110  0.2662403  0.2985941  0.2184950
      0.115  0.2662403  0.2985941  0.2184950
      0.120  0.2662403  0.2985941  0.2184950
      0.125  0.2662403  0.2985941  0.2184950
      0.130  0.2662403  0.2985941  0.2184950
      0.135  0.2662403  0.2985941  0.2184950
      0.140  0.2662403  0.2985941  0.2184950
      0.145  0.2662403  0.2985941  0.2184950
      0.150  0.2662403  0.2985941  0.2184950
    
    RMSE was used to select the optimal model using the smallest value.
    The final value used for the model was cp = 0.05.



```{r}
pred = predict(decision_tree_full_model, tst_data)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```


0.266362651090497



```{r}
pred = predict(decision_tree_reduced_model, tst_data)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```


0.258462408588737



```{r}
decision_tree_full_model_rmse = data.frame(decision_tree_full_model$results)
decision_tree_reduce_model_rmse = data.frame(decision_tree_reduced_model$results)

head(decision_tree_full_model_rmse,3)
```



```{r}
options(repr.plot.width=10, repr.plot.height=5)

ggplot(data = decision_tree_full_model_rmse, aes(x = cp, y = RMSE, color = 'Full Model')) +
  theme_update(plot.title = element_text(hjust = 0.5))+
  theme_set(theme_bw())+
  theme(text = element_text(size=14))+
  labs(title='Decision tree model Tuning', x='cp', y = 'Cross Validated RMSE')+
  scale_x_continuous(breaks = seq(0, 0.2, by = 0.02))+

  geom_line()+
  geom_point()+
    
  geom_line(aes(x = decision_tree_reduce_model_rmse$cp, y = decision_tree_reduce_model_rmse$RMSE, color = "Reduced Model"))+
  geom_point(aes(x = decision_tree_reduce_model_rmse$cp, y = decision_tree_reduce_model_rmse$RMSE, color = "Reduced Model"))
```



```{r}
pred = predict(decision_tree_full_model, tst_data)
```


```{r}
library(rattle)
fancyRpartPlot(decision_tree_full_model$finalModel)
```



    



```{r}
fancyRpartPlot(decision_tree_reduced_model$finalModel)
```



## XGBoost Model


```{r}
trn = as.matrix(trn_data[,1:(length(trn_data)-1)])
tst = as.matrix(tst_data[,1:(length(trn_data)-1)])
y = as.numeric(1 / (trn_data$Recovered+1))

m1_xgb <-
  xgboost::xgboost(
    data = trn,
    label = y,
    nrounds = 100,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 2,
    eta = .25
)
```

    [1]	train-rmse:0.287112 
    Will train until train_rmse hasn't improved in 3 rounds.
    
    [2]	train-rmse:0.257237 
    [3]	train-rmse:0.236319 
    [4]	train-rmse:0.221075 
    [5]	train-rmse:0.209131 
    [6]	train-rmse:0.200903 
    [7]	train-rmse:0.190656 
    [8]	train-rmse:0.183790 
    [9]	train-rmse:0.179291 
    [10]	train-rmse:0.172338 
    [11]	train-rmse:0.168037 
    [12]	train-rmse:0.164189 
    [13]	train-rmse:0.160637 
    [14]	train-rmse:0.158025 
    [15]	train-rmse:0.153943 
    [16]	train-rmse:0.150851 
    [17]	train-rmse:0.146700 
    [18]	train-rmse:0.142581 
    [19]	train-rmse:0.140686 
    [20]	train-rmse:0.136848 
    [21]	train-rmse:0.133945 
    [22]	train-rmse:0.130419 
    [23]	train-rmse:0.127407 
    [24]	train-rmse:0.125139 
    [25]	train-rmse:0.122012 
    [26]	train-rmse:0.120388 
    [27]	train-rmse:0.118952 
    [28]	train-rmse:0.116371 
    [29]	train-rmse:0.113865 
    [30]	train-rmse:0.111436 
    [31]	train-rmse:0.109167 
    [32]	train-rmse:0.106474 
    [33]	train-rmse:0.104879 
    [34]	train-rmse:0.101231 
    [35]	train-rmse:0.099641 
    [36]	train-rmse:0.098505 
    [37]	train-rmse:0.095885 
    [38]	train-rmse:0.093013 
    [39]	train-rmse:0.091217 
    [40]	train-rmse:0.089114 
    [41]	train-rmse:0.087732 
    [42]	train-rmse:0.085041 
    [43]	train-rmse:0.083279 
    [44]	train-rmse:0.082512 
    [45]	train-rmse:0.081337 
    [46]	train-rmse:0.080094 
    [47]	train-rmse:0.079325 
    [48]	train-rmse:0.078562 
    [49]	train-rmse:0.077197 
    [50]	train-rmse:0.075101 
    [51]	train-rmse:0.074573 
    [52]	train-rmse:0.072915 
    [53]	train-rmse:0.072319 
    [54]	train-rmse:0.070838 
    [55]	train-rmse:0.069448 
    [56]	train-rmse:0.067937 
    [57]	train-rmse:0.067441 
    [58]	train-rmse:0.066487 
    [59]	train-rmse:0.065947 
    [60]	train-rmse:0.065377 
    [61]	train-rmse:0.064116 
    [62]	train-rmse:0.062853 
    [63]	train-rmse:0.062033 
    [64]	train-rmse:0.061587 
    [65]	train-rmse:0.061179 
    [66]	train-rmse:0.059977 
    [67]	train-rmse:0.059153 
    [68]	train-rmse:0.058782 
    [69]	train-rmse:0.057525 
    [70]	train-rmse:0.055889 
    [71]	train-rmse:0.055360 
    [72]	train-rmse:0.054600 
    [73]	train-rmse:0.053407 
    [74]	train-rmse:0.052204 
    [75]	train-rmse:0.051651 
    [76]	train-rmse:0.051254 
    [77]	train-rmse:0.050295 
    [78]	train-rmse:0.048853 
    [79]	train-rmse:0.048068 
    [80]	train-rmse:0.047507 
    [81]	train-rmse:0.046603 
    [82]	train-rmse:0.045885 
    [83]	train-rmse:0.045578 
    [84]	train-rmse:0.044665 
    [85]	train-rmse:0.043870 
    [86]	train-rmse:0.043411 
    [87]	train-rmse:0.042746 
    [88]	train-rmse:0.042059 
    [89]	train-rmse:0.041403 
    [90]	train-rmse:0.040748 
    [91]	train-rmse:0.040483 
    [92]	train-rmse:0.039762 
    [93]	train-rmse:0.039383 
    [94]	train-rmse:0.039150 
    [95]	train-rmse:0.038457 
    [96]	train-rmse:0.037907 
    [97]	train-rmse:0.037666 
    [98]	train-rmse:0.036878 
    [99]	train-rmse:0.036354 
    [100]	train-rmse:0.035490 
    


```{r}
pred = predict(m1_xgb, tst)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```


0.249904967623603



```{r}
mat <- xgb.importance (feature_names = colnames(trn),model = m1_xgb)
xgb.plot.importance (importance_matrix = mat[1:20]) 
```


## Kernel Regression


```{r}
library('kernlab')
```


```{r}
set.seed(42)
model_Control = trainControl(method = "cv", number = 5)

kernel_regression_model_full = train(
  form = 1 / (Recovered+1) ~ .,
  data = trn_data,
  trControl = model_Control,
  method = "gaussprRadial",
  tuneGrid = expand.grid(sigma = seq(0.01, 0.1, 0.005))
)

kernel_regression_model_full
```


    Gaussian Process with Radial Basis Function Kernel 
    
    125 samples
     17 predictor
    
    No pre-processing
    Resampling: Cross-Validated (5 fold) 
    Summary of sample sizes: 100, 100, 101, 100, 99 
    Resampling results across tuning parameters:
    
      sigma  RMSE       Rsquared   MAE      
      0.010  0.2320687  0.4584832  0.1931997
      0.015  0.2306338  0.4630747  0.1901620
      0.020  0.2301543  0.4651479  0.1893383
      0.025  0.2302350  0.4653745  0.1895815
      0.030  0.2306714  0.4643397  0.1905678
      0.035  0.2313422  0.4624657  0.1918602
      0.040  0.2321736  0.4600269  0.1933074
      0.045  0.2331194  0.4571942  0.1949075
      0.050  0.2341495  0.4540746  0.1966059
      0.055  0.2352432  0.4507377  0.1982913
      0.060  0.2363852  0.4472322  0.1999548
      0.065  0.2375640  0.4435944  0.2015909
      0.070  0.2387701  0.4398538  0.2031958
      0.075  0.2399957  0.4360348  0.2047671
      0.080  0.2412341  0.4321589  0.2063676
      0.085  0.2424798  0.4282445  0.2079483
      0.090  0.2437279  0.4243081  0.2094967
      0.095  0.2449740  0.4203641  0.2110112
      0.100  0.2462146  0.4164248  0.2124907
    
    RMSE was used to select the optimal model using the smallest value.
    The final value used for the model was sigma = 0.02.



```{r}
set.seed(42)
model_Control = trainControl(method = "cv", number = 5)

kernel_regression_model_reduced = train(
  form = 1 / (Recovered+1) ~ Animal.fats + Sugar...Sweeteners + Oilcrops + Stimulants + Obesity + Eggs + Treenuts,
  data = trn_data,
  trControl = model_Control,
  method = "gaussprRadial",
  tuneGrid = expand.grid(sigma = seq(0.01, 0.1, 0.005))
)

kernel_regression_model_reduced
```


    Gaussian Process with Radial Basis Function Kernel 
    
    125 samples
      7 predictor
    
    No pre-processing
    Resampling: Cross-Validated (5 fold) 
    Summary of sample sizes: 100, 100, 101, 100, 99 
    Resampling results across tuning parameters:
    
      sigma  RMSE       Rsquared   MAE      
      0.010  0.2305560  0.4668290  0.1925810
      0.015  0.2292823  0.4682240  0.1889323
      0.020  0.2285790  0.4698643  0.1868722
      0.025  0.2281608  0.4710857  0.1855482
      0.030  0.2279424  0.4717579  0.1847221
      0.035  0.2278715  0.4719363  0.1842688
      0.040  0.2279078  0.4717267  0.1839940
      0.045  0.2280200  0.4712342  0.1838823
      0.050  0.2281844  0.4705463  0.1838968
      0.055  0.2283834  0.4697303  0.1839759
      0.060  0.2286047  0.4688358  0.1841403
      0.065  0.2288392  0.4678976  0.1843400
      0.070  0.2290810  0.4669393  0.1845635
      0.075  0.2293260  0.4659764  0.1848854
      0.080  0.2295717  0.4650185  0.1852134
      0.085  0.2298165  0.4640707  0.1855433
      0.090  0.2300595  0.4631354  0.1859222
      0.095  0.2303004  0.4622131  0.1863270
      0.100  0.2305392  0.4613028  0.1867641
    
    RMSE was used to select the optimal model using the smallest value.
    The final value used for the model was sigma = 0.035.



```{r}
pred = predict(kernel_regression_model_full, tst_data)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```


0.252345339280607



```{r}
pred = predict(kernel_regression_model_reduced, tst_data)
calc_RMSE(pred, 1/(tst_data$Recovered+1))
```


0.263082173498589



```{r}
kernel_full_model_rmse = data.frame(kernel_regression_model_full$results)
kernel_reduce_model_rmse = data.frame(kernel_regression_model_reduced$results)

head(kernel_full_model_rmse,3)
```



```{r}
options(repr.plot.width=10, repr.plot.height=5)

ggplot(data = kernel_full_model_rmse, aes(x = sigma, y = RMSE, color = 'Full Model')) +
  theme_update(plot.title = element_text(hjust = 0.5))+
  theme_set(theme_bw())+
  theme(text = element_text(size=14))+
  labs(title='Kernel Regression Model Tuning', x='Bandwidth', y = 'Cross Validated RMSE')+
  scale_x_continuous(breaks = seq(0, 0.2, by = 0.02))+

  geom_line()+
  geom_point()+
    
  geom_line(aes(x = kernel_reduce_model_rmse$sigma, y = kernel_reduce_model_rmse$RMSE, color = "Reduced Model"))+
  geom_point(aes(x = kernel_reduce_model_rmse$sigma, y = kernel_reduce_model_rmse$RMSE, color = "Reduced Model"))
```



